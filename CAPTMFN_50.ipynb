{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CopotronicRifat/CSE-437-PATTERN-RECOGNITION/blob/master/CAPTMFN_50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/jefferyYu/TomBERT.git\n",
        "!git clone https://github.com/Porky-Pig/TwitterImageData.git\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from transformers import RobertaModel, RobertaTokenizer, BertConfig\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import clip\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define paths to the data\n",
        "tsv_base_path = 'TomBERT/absa_data/twitter2015'\n",
        "image_base_path = 'TwitterImageData/twitter2015_images'\n",
        "\n",
        "# Verify paths\n",
        "print(\"TSV Directory exists:\", os.path.exists(tsv_base_path))\n",
        "print(\"Image Directory exists:\", os.path.exists(image_base_path))\n",
        "\n",
        "# Define the path to the base directory containing the TSV files\n",
        "columns = ['index', 'Label', 'ImageID', 'String1', 'String2']\n",
        "\n",
        "# Function to load and prepare data with the correct number of columns\n",
        "def load_and_prepare_data(filename):\n",
        "    file_path = os.path.join(tsv_base_path, filename)\n",
        "    return pd.read_csv(file_path, sep='\\t', header=0, names=columns)\n",
        "\n",
        "# Load the data files\n",
        "train_df = load_and_prepare_data('train.tsv')\n",
        "dev_df = load_and_prepare_data('dev.tsv')\n",
        "test_df = load_and_prepare_data('test.tsv')\n",
        "\n",
        "# Combine train and dev sets\n",
        "full_train_df = pd.concat([train_df, dev_df])\n",
        "\n",
        "# Split into new train and validation sets\n",
        "train_df, valid_df = train_test_split(full_train_df, test_size=0.1)\n",
        "\n",
        "# Initialize OpenAI CLIP model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_base_path, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.image_base_path = image_base_path\n",
        "        self.transform = transform\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "\n",
        "        tweet = row['String1']\n",
        "        aspect_term = row['String2']\n",
        "\n",
        "        inputs = self.tokenizer(tweet, return_tensors=\"pt\", padding='max_length', max_length=50, truncation=True)\n",
        "        input_ids = inputs['input_ids'].squeeze(0)\n",
        "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
        "\n",
        "        aspect_inputs = self.tokenizer(aspect_term, return_tensors=\"pt\", padding='max_length', max_length=20, truncation=True)\n",
        "        aspect_ids = aspect_inputs['input_ids'].squeeze(0)\n",
        "        aspect_attention_mask = aspect_inputs['attention_mask'].squeeze(0)\n",
        "\n",
        "        image_path = os.path.join(self.image_base_path, row['ImageID'])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        # Generate caption using CLIP\n",
        "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            text_inputs = clip.tokenize([\"This is a photo of\"]).to(device)\n",
        "            logits_per_image, logits_per_text = clip_model(image_input, text_inputs)\n",
        "            caption = clip.tokenize(\"This is a photo of\").to(device)\n",
        "\n",
        "        caption_inputs = self.tokenizer.decode(caption.squeeze(0).cpu().numpy())\n",
        "        caption_inputs = self.tokenizer(caption_inputs, return_tensors=\"pt\", padding='max_length', max_length=50, truncation=True)\n",
        "        caption_ids = caption_inputs['input_ids'].squeeze(0)\n",
        "        caption_attention_mask = caption_inputs['attention_mask'].squeeze(0)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = torch.tensor(row['Label'], dtype=torch.long)\n",
        "\n",
        "        return input_ids, attention_mask, aspect_ids, aspect_attention_mask, image, caption_ids, caption_attention_mask, label\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = CustomDataset(train_df, image_base_path, transform=transform)\n",
        "valid_dataset = CustomDataset(valid_df, image_base_path, transform=transform)\n",
        "test_dataset = CustomDataset(test_df, image_base_path, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "class TMFN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TMFN, self).__init__()\n",
        "        self.text_encoder = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.image_encoder = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.fusion_layer = nn.Linear(768 + 1000 + 768, 512)\n",
        "        self.classifier = nn.Linear(512, 3)\n",
        "\n",
        "        # Caption generation components\n",
        "        decoder_config = BertConfig.from_pretrained('roberta-base')\n",
        "        self.caption_decoder = RobertaModel(decoder_config)\n",
        "        self.caption_linear = nn.Linear(768, self.text_encoder.config.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, aspect_ids, aspect_attention_mask, images, caption_ids, caption_attention_mask):\n",
        "        text_features = self.text_encoder(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
        "        aspect_features = self.text_encoder(aspect_ids, attention_mask=aspect_attention_mask).last_hidden_state[:, 0, :]\n",
        "        image_features = self.image_encoder(images)\n",
        "        combined_features = torch.cat([text_features, aspect_features, image_features], dim=1)\n",
        "        fusion_output = torch.relu(self.fusion_layer(combined_features))\n",
        "        fusion_output = self.dropout(fusion_output)\n",
        "        logits = self.classifier(fusion_output)\n",
        "\n",
        "        # Caption generation\n",
        "        caption_outputs = self.caption_decoder(input_ids=caption_ids, attention_mask=caption_attention_mask)\n",
        "        caption_logits = self.caption_linear(caption_outputs.last_hidden_state)\n",
        "\n",
        "        return logits, caption_logits\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            input_ids, attention_mask, aspect_ids, aspect_attention_mask, images, caption_ids, caption_attention_mask, labels = [d.to(device) for d in data]\n",
        "            outputs, _ = model(input_ids, attention_mask, aspect_ids, aspect_attention_mask, images, caption_ids, caption_attention_mask)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return f1\n",
        "\n",
        "model = TMFN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "caption_criterion = nn.CrossEntropyLoss(ignore_index=model.text_encoder.config.pad_token_id)\n",
        "\n",
        "best_valid_f1 = 0\n",
        "classification_weight = 0.5\n",
        "caption_weight = 0.5\n",
        "\n",
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in tqdm(train_loader):\n",
        "        input_ids, attention_mask, aspect_ids, aspect_attention_mask, images, caption_ids, caption_attention_mask, labels = [d.to(device) for d in data]\n",
        "        outputs, caption_logits = model(input_ids, attention_mask, aspect_ids, aspect_attention_mask, images, caption_ids, caption_attention_mask)\n",
        "\n",
        "        loss = classification_weight * criterion(outputs, labels)\n",
        "        caption_loss = caption_weight * caption_criterion(caption_logits.view(-1, caption_logits.size(-1)), caption_ids.view(-1))\n",
        "        total_loss = loss + caption_loss\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    valid_f1 = evaluate_model(model, valid_loader, device)\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}, Validation F1: {valid_f1}')\n",
        "\n",
        "    if valid_f1 > best_valid_f1:\n",
        "        best_valid_f1 = valid_f1\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_f1 = evaluate_model(model, test_loader, device)\n",
        "print(f'Test F1 Score: {test_f1}')\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-06-07T05:59:30.283183Z",
          "iopub.execute_input": "2024-06-07T05:59:30.283941Z",
          "iopub.status.idle": "2024-06-07T10:15:37.619817Z",
          "shell.execute_reply.started": "2024-06-07T05:59:30.283906Z",
          "shell.execute_reply": "2024-06-07T10:15:37.618706Z"
        },
        "trusted": true,
        "id": "JAtJRSC5i9JU",
        "outputId": "3c27bcc6-6b74-4ee4-d0bd-305f3e37b4b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-xrmncvsl\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-xrmncvsl\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (6.2.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (21.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.16.2)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->clip==1.0) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2024.3.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nfatal: destination path 'TomBERT' already exists and is not an empty directory.\nfatal: destination path 'TwitterImageData' already exists and is not an empty directory.\nTSV Directory exists: True\nImage Directory exists: True\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nYou are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n100%|██████████| 484/484 [04:46<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1, Loss: 0.0008295896695926785, Validation F1: 0.6349194764754538\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:45<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2, Loss: 0.00048632282414473593, Validation F1: 0.6982129390217625\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:45<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3, Loss: 0.0004549459263216704, Validation F1: 0.7191830767522575\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:45<00:00,  1.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4, Loss: 0.00015855552919674665, Validation F1: 0.7589695574428399\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:48<00:00,  1.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5, Loss: 0.00010455430310685188, Validation F1: 0.7228486873300232\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:46<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6, Loss: 0.0003452126693446189, Validation F1: 0.7173499888419679\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:45<00:00,  1.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7, Loss: 9.623046025808435e-06, Validation F1: 0.7345761081094148\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:45<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 8, Loss: 3.6708031984744594e-05, Validation F1: 0.7265483934555489\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:45<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 9, Loss: 4.473803073778981e-06, Validation F1: 0.7533645870319217\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:45<00:00,  1.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 10, Loss: 2.1982890757499263e-05, Validation F1: 0.7048462969040585\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:47<00:00,  1.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 11, Loss: 7.256073786265915e-06, Validation F1: 0.728678835310906\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:44<00:00,  1.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 12, Loss: 2.6760742457554443e-06, Validation F1: 0.7297536150533306\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:44<00:00,  1.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 13, Loss: 0.00012894318206235766, Validation F1: 0.7079488639673462\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:44<00:00,  1.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 14, Loss: 4.162201094004558e-06, Validation F1: 0.7429977576087378\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:46<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 15, Loss: 3.6966216612199787e-06, Validation F1: 0.7302840038948427\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:46<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 16, Loss: 1.1011957212758716e-06, Validation F1: 0.7002001918736518\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:45<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 17, Loss: 3.215950982848881e-06, Validation F1: 0.7366499136583063\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:47<00:00,  1.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 18, Loss: 4.1866744140861556e-05, Validation F1: 0.7291937311976014\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:47<00:00,  1.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 19, Loss: 6.71169516408554e-07, Validation F1: 0.7321018761543696\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:45<00:00,  1.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 20, Loss: 2.2682881990476744e-06, Validation F1: 0.7161946259985476\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:44<00:00,  1.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 21, Loss: 2.7094951292383485e-05, Validation F1: 0.7207270243424766\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:45<00:00,  1.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 22, Loss: 6.000641405989882e-07, Validation F1: 0.7181553477690289\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:45<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 23, Loss: 6.886801173777712e-08, Validation F1: 0.7333816370142762\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:45<00:00,  1.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 24, Loss: 1.6114514664877788e-06, Validation F1: 0.7182230467944754\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:45<00:00,  1.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 25, Loss: 3.0294188491097884e-07, Validation F1: 0.734823320928584\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:45<00:00,  1.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 26, Loss: 9.634550951886922e-05, Validation F1: 0.7159037893372572\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:45<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 27, Loss: 3.764264704386733e-07, Validation F1: 0.6995168626464608\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:46<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 28, Loss: 1.7827267129177926e-06, Validation F1: 0.7142484653435842\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:46<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 29, Loss: 6.483476511220942e-08, Validation F1: 0.732127317640686\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:46<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 30, Loss: 7.4396443778823595e-06, Validation F1: 0.7178080387856897\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:46<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 31, Loss: 3.8790490179962944e-06, Validation F1: 0.7202515836178199\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:46<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 32, Loss: 3.7069586511506714e-08, Validation F1: 0.7359721201230635\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:46<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 33, Loss: 1.666096068220213e-06, Validation F1: 0.7073173813545037\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:46<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 34, Loss: 1.6040936543504358e-06, Validation F1: 0.6940103666115659\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:47<00:00,  1.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 35, Loss: 0.0002375784097239375, Validation F1: 0.7220474960009845\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:46<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 36, Loss: 2.446004430112225e-07, Validation F1: 0.7497917082272316\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:48<00:00,  1.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 37, Loss: 8.825801387501997e-07, Validation F1: 0.7505729014620867\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:51<00:00,  1.66it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 38, Loss: 1.2616567346412921e-07, Validation F1: 0.7396985852431396\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:51<00:00,  1.66it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 39, Loss: 4.313857516535791e-06, Validation F1: 0.735254170753804\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:50<00:00,  1.66it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 40, Loss: 1.3557186093748896e-06, Validation F1: 0.7375979929120179\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:50<00:00,  1.67it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 41, Loss: 3.138759439025307e-06, Validation F1: 0.7396123633205699\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:50<00:00,  1.67it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 42, Loss: 0.0013165527489036322, Validation F1: 0.7457433027371835\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:50<00:00,  1.66it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 43, Loss: 8.808227721601725e-05, Validation F1: 0.7334069677554603\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:51<00:00,  1.66it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 44, Loss: 6.983396474424808e-07, Validation F1: 0.7277967834532656\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:51<00:00,  1.66it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 45, Loss: 1.9094193248747615e-06, Validation F1: 0.7236427967891846\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:47<00:00,  1.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 46, Loss: 1.0706709190344554e-06, Validation F1: 0.7466362797123685\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:46<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 47, Loss: 2.4775399509735507e-08, Validation F1: 0.7424501921379446\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:46<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 48, Loss: 0.0007479292689822614, Validation F1: 0.7279840353622796\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:47<00:00,  1.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 49, Loss: 0.00010704964370233938, Validation F1: 0.6852249059351863\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 484/484 [04:46<00:00,  1.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 50, Loss: 9.002966407933854e-07, Validation F1: 0.7492087112622827\nTest F1 Score: 0.7407656636499418\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}